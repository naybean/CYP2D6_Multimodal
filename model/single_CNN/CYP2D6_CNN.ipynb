{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276bc389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve, roc_auc_score, average_precision_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f84a251c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 사용 가능 여부: True\n"
     ]
    }
   ],
   "source": [
    "#GPU 사용 가능 여부 확인\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "print('GPU 사용 가능 여부:', USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e534b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습을 진행하는 기기: cuda:0\n"
     ]
    }
   ],
   "source": [
    "#사용할 디바이스 설정 (GPU 또는 CPU)\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "print('학습을 진행하는 기기:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57c46f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'de', 1: 'f', 2: 'nor'}\n"
     ]
    }
   ],
   "source": [
    "# CSV 파일 로드\n",
    "train_data = pd.read_csv('f_train_55.csv')\n",
    "\n",
    "# 'Class' 열 숫자 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "train_data['Class'] = label_encoder.fit_transform(train_data['Class'])\n",
    "class_mapping = {index: label for index, label in enumerate(label_encoder.classes_)}\n",
    "print(class_mapping)\n",
    "\n",
    "# 테스트 데이터 로드\n",
    "test_data = pd.read_csv('f_test_149.csv')\n",
    "\n",
    "# 'Class' 열 숫자 인코딩 (훈련 데이터에서 학습한 label_encoder 사용)\n",
    "test_data['Class'] = label_encoder.transform(test_data['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0481307a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CYP2D6.106.002: shape (4312, 7)\n",
      "Loaded CYP2D6.35.004: shape (4312, 7)\n",
      "Loaded CYP2D6.2.004: shape (4312, 7)\n",
      "Loaded CYP2D6.4.012: shape (4312, 7)\n",
      "Loaded CYP2D6.36.002: shape (4312, 7)\n",
      "Loaded CYP2D6.10.003: shape (4312, 7)\n",
      "Loaded CYP2D6.1.050: shape (4312, 7)\n",
      "Loaded CYP2D6.105.001: shape (4312, 7)\n",
      "Loaded CYP2D6.159.001: shape (4312, 7)\n",
      "Loaded CYP2D6.134.001: shape (4312, 7)\n",
      "Loaded CYP2D6.155.001: shape (4312, 7)\n",
      "Loaded CYP2D6.4.015: shape (4312, 7)\n",
      "Loaded CYP2D6.11.001: shape (4312, 7)\n",
      "Loaded CYP2D6.71.001: shape (4312, 7)\n",
      "Loaded CYP2D6.4.003: shape (4312, 7)\n",
      "Loaded CYP2D6.47.001: shape (4312, 7)\n",
      "Loaded CYP2D6.4.010: shape (4312, 7)\n",
      "Loaded CYP2D6.4.024: shape (4312, 7)\n",
      "Loaded CYP2D6.2.015: shape (4312, 7)\n",
      "Loaded CYP2D6.164.001: shape (4312, 7)\n",
      "Loaded CYP2D6.85.001: shape (4312, 7)\n",
      "Loaded CYP2D6.3.002: shape (4312, 7)\n",
      "Loaded CYP2D6.1.035: shape (4312, 7)\n",
      "Loaded CYP2D6.3.003: shape (4312, 7)\n",
      "Loaded CYP2D6.97.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.013: shape (4312, 7)\n",
      "Loaded CYP2D6.4.004: shape (4312, 7)\n",
      "Loaded CYP2D6.2.025: shape (4312, 7)\n",
      "Loaded CYP2D6.1.030: shape (4312, 7)\n",
      "Loaded CYP2D6.1.053: shape (4318, 7)\n",
      "Loaded CYP2D6.2.027: shape (4312, 7)\n",
      "Loaded CYP2D6.46.002: shape (4312, 7)\n",
      "Loaded CYP2D6.1.057: shape (4312, 7)\n",
      "Loaded CYP2D6.20.001: shape (4313, 7)\n",
      "Loaded CYP2D6.153.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.058: shape (4312, 7)\n",
      "Loaded CYP2D6.4.009: shape (4312, 7)\n",
      "Loaded CYP2D6.162.001: shape (4312, 7)\n",
      "Loaded CYP2D6.154.001: shape (4312, 7)\n",
      "Loaded CYP2D6.83.001: shape (4312, 7)\n",
      "Loaded CYP2D6.45.003: shape (4312, 7)\n",
      "Loaded CYP2D6.41.001: shape (4312, 7)\n",
      "Loaded CYP2D6.6.005: shape (4312, 7)\n",
      "Loaded CYP2D6.4.034: shape (4312, 7)\n",
      "Loaded CYP2D6.19.001: shape (4312, 7)\n",
      "Loaded CYP2D6.132.001: shape (4312, 7)\n",
      "Loaded CYP2D6.83.003: shape (4312, 7)\n",
      "Loaded CYP2D6.114.001: shape (4312, 7)\n",
      "Loaded CYP2D6.22.001: shape (4312, 7)\n",
      "Loaded CYP2D6.4.026: shape (4312, 7)\n",
      "Loaded CYP2D6.127.001: shape (4313, 7)\n",
      "Loaded CYP2D6.1.031: shape (4312, 7)\n",
      "Loaded CYP2D6.152.001: shape (4312, 7)\n",
      "Loaded CYP2D6.143.001: shape (4312, 7)\n",
      "Loaded CYP2D6.92.001: shape (4313, 7)\n",
      "Loaded CYP2D6.4.001: shape (4312, 7)\n",
      "Loaded CYP2D6.36.003: shape (4312, 7)\n",
      "Loaded CYP2D6.45.002: shape (4312, 7)\n",
      "Loaded CYP2D6.1.028: shape (4312, 7)\n",
      "Loaded CYP2D6.48.001: shape (4312, 7)\n",
      "Loaded CYP2D6.4.019: shape (4312, 7)\n",
      "Loaded CYP2D6.112.001: shape (4312, 7)\n",
      "Loaded CYP2D6.147.001: shape (4321, 7)\n",
      "Loaded CYP2D6.1.023: shape (4312, 7)\n",
      "Loaded CYP2D6.10.004: shape (4312, 7)\n",
      "Loaded CYP2D6.90.001: shape (4312, 7)\n",
      "Loaded CYP2D6.4.007: shape (4312, 7)\n",
      "Loaded CYP2D6.10.005: shape (4312, 7)\n",
      "Loaded CYP2D6.2.009: shape (4312, 7)\n",
      "Loaded CYP2D6.150.002: shape (4313, 7)\n",
      "Loaded CYP2D6.1.059: shape (4312, 7)\n",
      "Loaded CYP2D6.103.001: shape (4312, 7)\n",
      "Loaded CYP2D6.17.006: shape (4312, 7)\n",
      "Loaded CYP2D6.2.013: shape (4312, 7)\n",
      "Loaded CYP2D6.10.007: shape (4312, 7)\n",
      "Loaded CYP2D6.21.002: shape (4313, 7)\n",
      "Loaded CYP2D6.1.018: shape (4312, 7)\n",
      "Loaded CYP2D6.2.031: shape (4312, 7)\n",
      "Loaded CYP2D6.141.001: shape (4313, 7)\n",
      "Loaded CYP2D6.1.015: shape (4312, 7)\n",
      "Loaded CYP2D6.2.020: shape (4312, 7)\n",
      "Loaded CYP2D6.1.022: shape (4312, 7)\n",
      "Loaded CYP2D6.1.012: shape (4312, 7)\n",
      "Loaded CYP2D6.1.008: shape (4312, 7)\n",
      "Loaded CYP2D6.144.001: shape (4312, 7)\n",
      "Loaded CYP2D6.4.013: shape (4312, 7)\n",
      "Loaded CYP2D6.4.030: shape (4312, 7)\n",
      "Loaded CYP2D6.4.006: shape (4312, 7)\n",
      "Loaded CYP2D6.6.006: shape (4312, 7)\n",
      "Loaded CYP2D6.156.001: shape (4312, 7)\n",
      "Loaded CYP2D6.124.002: shape (4312, 7)\n",
      "Loaded CYP2D6.10.006: shape (4312, 7)\n",
      "Loaded CYP2D6.27.003: shape (4312, 7)\n",
      "Loaded CYP2D6.87.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.055: shape (4312, 7)\n",
      "Loaded CYP2D6.1.032: shape (4312, 7)\n",
      "Loaded CYP2D6.18.001: shape (4321, 7)\n",
      "Loaded CYP2D6.17.007: shape (4312, 7)\n",
      "Loaded CYP2D6.2.002: shape (4312, 7)\n",
      "Loaded CYP2D6.94.002: shape (4312, 7)\n",
      "Loaded CYP2D6.43.001: shape (4312, 7)\n",
      "Loaded CYP2D6.148.001: shape (4312, 7)\n",
      "Loaded CYP2D6.82.001: shape (4312, 7)\n",
      "Loaded CYP2D6.98.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.040: shape (4312, 7)\n",
      "Loaded CYP2D6.49.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.026: shape (4312, 7)\n",
      "Loaded CYP2D6.106.001: shape (4312, 7)\n",
      "Loaded CYP2D6.36.004: shape (4312, 7)\n",
      "Loaded CYP2D6.2.018: shape (4312, 7)\n",
      "Loaded CYP2D6.7.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.052: shape (4312, 7)\n",
      "Loaded CYP2D6.1.041: shape (4312, 7)\n",
      "Loaded CYP2D6.2.010: shape (4312, 7)\n",
      "Loaded CYP2D6.2.005: shape (4312, 7)\n",
      "Loaded CYP2D6.1.036: shape (4312, 7)\n",
      "Loaded CYP2D6.25.001: shape (4312, 7)\n",
      "Loaded CYP2D6.158.001: shape (4312, 7)\n",
      "Loaded CYP2D6.4.020: shape (4312, 7)\n",
      "Loaded CYP2D6.1.047: shape (4312, 7)\n",
      "Loaded CYP2D6.175.001: shape (4312, 7)\n",
      "Loaded CYP2D6.46.001: shape (4312, 7)\n",
      "Loaded CYP2D6.89.001: shape (4312, 7)\n",
      "Loaded CYP2D6.41.002: shape (4312, 7)\n",
      "Loaded CYP2D6.124.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.056: shape (4312, 7)\n",
      "Loaded CYP2D6.50.001: shape (4312, 7)\n",
      "Loaded CYP2D6.28.001: shape (4312, 7)\n",
      "Loaded CYP2D6.2.032: shape (4312, 7)\n",
      "Loaded CYP2D6.28.002: shape (4312, 7)\n",
      "Loaded CYP2D6.4.031: shape (4312, 7)\n",
      "Loaded CYP2D6.35.002: shape (4312, 7)\n",
      "Loaded CYP2D6.1.049: shape (4312, 7)\n",
      "Loaded CYP2D6.60.001: shape (4314, 7)\n",
      "Loaded CYP2D6.73.001: shape (4312, 7)\n",
      "Loaded CYP2D6.4.027: shape (4312, 7)\n",
      "Loaded CYP2D6.34.001: shape (4312, 7)\n",
      "Loaded CYP2D6.75.001: shape (4312, 7)\n",
      "Loaded CYP2D6.145.001: shape (4321, 7)\n",
      "Loaded CYP2D6.2.029: shape (4312, 7)\n",
      "Loaded CYP2D6.1.033: shape (4312, 7)\n",
      "Loaded CYP2D6.35.007: shape (4312, 7)\n",
      "Loaded CYP2D6.28.003: shape (4312, 7)\n",
      "Loaded CYP2D6.100.001: shape (4312, 7)\n",
      "Loaded CYP2D6.165.001: shape (4312, 7)\n",
      "Loaded CYP2D6.21.001: shape (4313, 7)\n",
      "Loaded CYP2D6.8.001: shape (4312, 7)\n",
      "Loaded CYP2D6.54.001: shape (4312, 7)\n",
      "Loaded CYP2D6.84.001: shape (4312, 7)\n",
      "Loaded CYP2D6.133.001: shape (4312, 7)\n",
      "Loaded CYP2D6.41.006: shape (4312, 7)\n",
      "Loaded CYP2D6.113.001: shape (4312, 7)\n",
      "Loaded CYP2D6.116.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.025: shape (4312, 7)\n",
      "Loaded CYP2D6.139.001: shape (4312, 7)\n",
      "Loaded CYP2D6.17.001: shape (4312, 7)\n",
      "Loaded CYP2D6.72.001: shape (4312, 7)\n",
      "Loaded CYP2D6.169.001: shape (4312, 7)\n",
      "Loaded CYP2D6.2.030: shape (4312, 7)\n",
      "Loaded CYP2D6.2.003: shape (4312, 7)\n",
      "Loaded CYP2D6.17.004: shape (4312, 7)\n",
      "Loaded CYP2D6.46.003: shape (4312, 7)\n",
      "Loaded CYP2D6.1.007: shape (4313, 7)\n",
      "Loaded CYP2D6.1.037: shape (4312, 7)\n",
      "Loaded CYP2D6.56.001: shape (4312, 7)\n",
      "Loaded CYP2D6.62.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.054: shape (4312, 7)\n",
      "Loaded CYP2D6.111.001: shape (4312, 7)\n",
      "Loaded CYP2D6.131.001: shape (4312, 7)\n",
      "Loaded CYP2D6.129.001: shape (4312, 7)\n",
      "Loaded CYP2D6.2.026: shape (4312, 7)\n",
      "Loaded CYP2D6.142.001: shape (4312, 7)\n",
      "Loaded CYP2D6.4.016: shape (4312, 7)\n",
      "Loaded CYP2D6.4.018: shape (4312, 7)\n",
      "Loaded CYP2D6.35.003: shape (4312, 7)\n",
      "Loaded CYP2D6.1.027: shape (4312, 7)\n",
      "Loaded CYP2D6.1.042: shape (4312, 7)\n",
      "Loaded CYP2D6.1.046: shape (4312, 7)\n",
      "Loaded CYP2D6.157.001: shape (4313, 7)\n",
      "Loaded CYP2D6.53.001: shape (4312, 7)\n",
      "Loaded CYP2D6.102.001: shape (4312, 7)\n",
      "Loaded CYP2D6.17.002: shape (4312, 7)\n",
      "Loaded CYP2D6.1.001: shape (4312, 7)\n",
      "Loaded CYP2D6.58.001: shape (4321, 7)\n",
      "Loaded CYP2D6.39.001: shape (4312, 7)\n",
      "Loaded CYP2D6.6.004: shape (4312, 7)\n",
      "Loaded CYP2D6.1.044: shape (4312, 7)\n",
      "Loaded CYP2D6.160.001: shape (4312, 7)\n",
      "Loaded CYP2D6.9.002: shape (4312, 7)\n",
      "Loaded CYP2D6.44.001: shape (4312, 7)\n",
      "Loaded CYP2D6.125.001: shape (4312, 7)\n",
      "Loaded CYP2D6.2.024: shape (4312, 7)\n",
      "Loaded CYP2D6.104.001: shape (4312, 7)\n",
      "Loaded CYP2D6.136.001: shape (4312, 7)\n",
      "Loaded CYP2D6.4.022: shape (4312, 7)\n",
      "Loaded CYP2D6.123.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.048: shape (4312, 7)\n",
      "Loaded CYP2D6.81.001: shape (4312, 7)\n",
      "Loaded CYP2D6.121.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.009: shape (4312, 7)\n",
      "Loaded CYP2D6.52.002: shape (4312, 7)\n",
      "Loaded CYP2D6.4.005: shape (4312, 7)\n",
      "Loaded CYP2D6.1.039: shape (4312, 7)\n",
      "Loaded CYP2D6.137.001: shape (4312, 7)\n",
      "Loaded CYP2D6.23.001: shape (4312, 7)\n",
      "Loaded CYP2D6.106.003: shape (4312, 7)\n",
      "Loaded CYP2D6.111.002: shape (4312, 7)\n",
      "Loaded CYP2D6.4.032: shape (4312, 7)\n",
      "Loaded CYP2D6.119.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.045: shape (4312, 7)\n",
      "Loaded CYP2D6.173.001: shape (4312, 7)\n",
      "Loaded CYP2D6.12.002: shape (4312, 7)\n",
      "Loaded CYP2D6.2.019: shape (4312, 7)\n",
      "Loaded CYP2D6.4.002: shape (4312, 7)\n",
      "Loaded CYP2D6.172.001: shape (4312, 7)\n",
      "Loaded CYP2D6.40.002: shape (4330, 7)\n",
      "Loaded CYP2D6.151.001: shape (4312, 7)\n",
      "Loaded CYP2D6.122.001: shape (4312, 7)\n",
      "Loaded CYP2D6.4.025: shape (4312, 7)\n",
      "Loaded CYP2D6.94.001: shape (4312, 7)\n",
      "Loaded CYP2D6.2.017: shape (4312, 7)\n",
      "Loaded CYP2D6.138.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.010: shape (4312, 7)\n",
      "Loaded CYP2D6.65.001: shape (4312, 7)\n",
      "Loaded CYP2D6.74.001: shape (4312, 7)\n",
      "Loaded CYP2D6.37.001: shape (4312, 7)\n",
      "Loaded CYP2D6.3.001: shape (4312, 7)\n",
      "Loaded CYP2D6.149.001: shape (4312, 7)\n",
      "Loaded CYP2D6.130.001: shape (4312, 7)\n",
      "Loaded CYP2D6.15.003: shape (4313, 7)\n",
      "Loaded CYP2D6.1.004: shape (4312, 7)\n",
      "Loaded CYP2D6.19.002: shape (4312, 7)\n",
      "Loaded CYP2D6.2.021: shape (4312, 7)\n",
      "Loaded CYP2D6.35.006: shape (4312, 7)\n",
      "Loaded CYP2D6.88.001: shape (4312, 7)\n",
      "Loaded CYP2D6.38.001: shape (4312, 7)\n",
      "Loaded CYP2D6.110.001: shape (4312, 7)\n",
      "Loaded CYP2D6.4.017: shape (4312, 7)\n",
      "Loaded CYP2D6.36.005: shape (4312, 7)\n",
      "Loaded CYP2D6.99.001: shape (4312, 7)\n",
      "Loaded CYP2D6.17.005: shape (4312, 7)\n",
      "Loaded CYP2D6.1.021: shape (4312, 7)\n",
      "Loaded CYP2D6.1.005: shape (4312, 7)\n",
      "Loaded CYP2D6.51.001: shape (4312, 7)\n",
      "Loaded CYP2D6.17.003: shape (4312, 7)\n",
      "Loaded CYP2D6.2.033: shape (4312, 7)\n",
      "Loaded CYP2D6.1.034: shape (4312, 7)\n",
      "Loaded CYP2D6.64.001: shape (4312, 7)\n",
      "Loaded CYP2D6.10.001: shape (4312, 7)\n",
      "Loaded CYP2D6.31.001: shape (4312, 7)\n",
      "Loaded CYP2D6.33.002: shape (4312, 7)\n",
      "Loaded CYP2D6.126.001: shape (4312, 7)\n",
      "Loaded CYP2D6.2.011: shape (4312, 7)\n",
      "Loaded CYP2D6.1.024: shape (4312, 7)\n",
      "Loaded CYP2D6.59.001: shape (4312, 7)\n",
      "Loaded CYP2D6.93.001: shape (4312, 7)\n",
      "Loaded CYP2D6.41.003: shape (4312, 7)\n",
      "Loaded CYP2D6.10.002: shape (4312, 7)\n",
      "Loaded CYP2D6.14.001: shape (4312, 7)\n",
      "Loaded CYP2D6.40.001: shape (4330, 7)\n",
      "Loaded CYP2D6.2.012: shape (4312, 7)\n",
      "Loaded CYP2D6.4.011: shape (4312, 7)\n",
      "Loaded CYP2D6.135.001: shape (4312, 7)\n",
      "Loaded CYP2D6.2.028: shape (4312, 7)\n",
      "Loaded CYP2D6.84.002: shape (4312, 7)\n",
      "Loaded CYP2D6.55.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.003: shape (4312, 7)\n",
      "Loaded CYP2D6.1.029: shape (4312, 7)\n",
      "Loaded CYP2D6.2.008: shape (4312, 7)\n",
      "Loaded CYP2D6.146.002: shape (4312, 7)\n",
      "Loaded CYP2D6.109.001: shape (4312, 7)\n",
      "Loaded CYP2D6.171.001: shape (4313, 7)\n",
      "Loaded CYP2D6.2.001: shape (4312, 7)\n",
      "Loaded CYP2D6.15.002: shape (4313, 7)\n",
      "Loaded CYP2D6.71.003: shape (4312, 7)\n",
      "Loaded CYP2D6.41.005: shape (4312, 7)\n",
      "Loaded CYP2D6.35.001: shape (4312, 7)\n",
      "Loaded CYP2D6.167.001: shape (4312, 7)\n",
      "Loaded CYP2D6.101.001: shape (4312, 7)\n",
      "Loaded CYP2D6.168.001: shape (4312, 7)\n",
      "Loaded CYP2D6.120.001: shape (4312, 7)\n",
      "Loaded CYP2D6.41.004: shape (4312, 7)\n",
      "Loaded CYP2D6.1.017: shape (4312, 7)\n",
      "Loaded CYP2D6.56.003: shape (4312, 7)\n",
      "Loaded CYP2D6.42.001: shape (4314, 7)\n",
      "Loaded CYP2D6.4.028: shape (4312, 7)\n",
      "Loaded CYP2D6.75.002: shape (4312, 7)\n",
      "Loaded CYP2D6.9.001: shape (4312, 7)\n",
      "Loaded CYP2D6.128.001: shape (4312, 7)\n",
      "Loaded CYP2D6.2.016: shape (4312, 7)\n",
      "Loaded CYP2D6.32.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.060: shape (4312, 7)\n",
      "Loaded CYP2D6.4.023: shape (4312, 7)\n",
      "Loaded CYP2D6.10.008: shape (4312, 7)\n",
      "Loaded CYP2D6.1.019: shape (4312, 7)\n",
      "Loaded CYP2D6.35.005: shape (4312, 7)\n",
      "Loaded CYP2D6.1.038: shape (4312, 7)\n",
      "Loaded CYP2D6.1.020: shape (4312, 7)\n",
      "Loaded CYP2D6.166.001: shape (4312, 7)\n",
      "Loaded CYP2D6.83.002: shape (4312, 7)\n",
      "Loaded CYP2D6.45.004: shape (4312, 7)\n",
      "Loaded CYP2D6.69.001: shape (4312, 7)\n",
      "Loaded CYP2D6.2.007: shape (4312, 7)\n",
      "Loaded CYP2D6.27.002: shape (4312, 7)\n",
      "Loaded CYP2D6.71.002: shape (4312, 7)\n",
      "Loaded CYP2D6.91.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.011: shape (4312, 7)\n",
      "Loaded CYP2D6.52.001: shape (4312, 7)\n",
      "Loaded CYP2D6.56.002: shape (4312, 7)\n",
      "Loaded CYP2D6.4.014: shape (4312, 7)\n",
      "Loaded CYP2D6.33.001: shape (4312, 7)\n",
      "Loaded CYP2D6.15.001: shape (4313, 7)\n",
      "Loaded CYP2D6.146.001: shape (4313, 7)\n",
      "Loaded CYP2D6.170.001: shape (4312, 7)\n",
      "Loaded CYP2D6.30.001: shape (4321, 7)\n",
      "Loaded CYP2D6.1.014: shape (4313, 7)\n",
      "Loaded CYP2D6.6.003: shape (4312, 7)\n",
      "Loaded CYP2D6.108.001: shape (4312, 7)\n",
      "Loaded CYP2D6.2.014: shape (4312, 7)\n",
      "Loaded CYP2D6.118.001: shape (4312, 7)\n",
      "Loaded CYP2D6.6.001: shape (4312, 7)\n",
      "Loaded CYP2D6.4.008: shape (4312, 7)\n",
      "Loaded CYP2D6.24.001: shape (4312, 7)\n",
      "Loaded CYP2D6.86.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.063: shape (4312, 7)\n",
      "Loaded CYP2D6.163.001: shape (4312, 7)\n",
      "Loaded CYP2D6.140.001: shape (4312, 7)\n",
      "Loaded CYP2D6.96.001: shape (4312, 7)\n",
      "Loaded CYP2D6.43.002: shape (4312, 7)\n",
      "Loaded CYP2D6.2.034: shape (4312, 7)\n",
      "Loaded CYP2D6.26.001: shape (4312, 7)\n",
      "Loaded CYP2D6.150.001: shape (4313, 7)\n",
      "Loaded CYP2D6.45.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.062: shape (4312, 7)\n",
      "Loaded CYP2D6.1.002: shape (4312, 7)\n",
      "Loaded CYP2D6.27.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.006: shape (4312, 7)\n",
      "Loaded CYP2D6.5.001: shape (4312, 7)\n",
      "Loaded CYP2D6.29.001: shape (4312, 7)\n",
      "Loaded CYP2D6.6.002: shape (4312, 7)\n",
      "Loaded CYP2D6.4.021: shape (4312, 7)\n",
      "Loaded CYP2D6.70.001: shape (4312, 7)\n",
      "Loaded CYP2D6.95.001: shape (4312, 7)\n",
      "Loaded CYP2D6.41.007: shape (4312, 7)\n",
      "Loaded CYP2D6.2.022: shape (4312, 7)\n",
      "Loaded CYP2D6.1.061: shape (4312, 7)\n",
      "Loaded CYP2D6.174.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.051: shape (4312, 7)\n",
      "Loaded CYP2D6.161.001: shape (4313, 7)\n",
      "Loaded CYP2D6.12.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.043: shape (4312, 7)\n",
      "Loaded CYP2D6.107.001: shape (4312, 7)\n",
      "Loaded CYP2D6.2.006: shape (4312, 7)\n",
      "Loaded CYP2D6.117.001: shape (4312, 7)\n",
      "Loaded CYP2D6.36.001: shape (4312, 7)\n",
      "Loaded CYP2D6.2.023: shape (4312, 7)\n",
      "Loaded CYP2D6.4.029: shape (4312, 7)\n",
      "Loaded CYP2D6.115.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.016: shape (4312, 7)\n",
      "Loaded CYP2D6.39.002: shape (4312, 7)\n",
      "Loaded CYP2D6.4.033: shape (4312, 7)\n",
      "최대 행 수: 4330\n",
      "로드된 매트릭스 개수: 361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CYP2D6.2.012: shape (4312, 7)\n",
      "Loaded CYP2D6.4.011: shape (4312, 7)\n",
      "Loaded CYP2D6.135.001: shape (4312, 7)\n",
      "Loaded CYP2D6.2.028: shape (4312, 7)\n",
      "Loaded CYP2D6.84.002: shape (4312, 7)\n",
      "Loaded CYP2D6.55.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.003: shape (4312, 7)\n",
      "Loaded CYP2D6.1.029: shape (4312, 7)\n",
      "Loaded CYP2D6.2.008: shape (4312, 7)\n",
      "Loaded CYP2D6.146.002: shape (4312, 7)\n",
      "Loaded CYP2D6.109.001: shape (4312, 7)\n",
      "Loaded CYP2D6.171.001: shape (4313, 7)\n",
      "Loaded CYP2D6.2.001: shape (4312, 7)\n",
      "Loaded CYP2D6.15.002: shape (4313, 7)\n",
      "Loaded CYP2D6.71.003: shape (4312, 7)\n",
      "Loaded CYP2D6.41.005: shape (4312, 7)\n",
      "Loaded CYP2D6.35.001: shape (4312, 7)\n",
      "Loaded CYP2D6.167.001: shape (4312, 7)\n",
      "Loaded CYP2D6.101.001: shape (4312, 7)\n",
      "Loaded CYP2D6.168.001: shape (4312, 7)\n",
      "Loaded CYP2D6.120.001: shape (4312, 7)\n",
      "Loaded CYP2D6.41.004: shape (4312, 7)\n",
      "Loaded CYP2D6.1.017: shape (4312, 7)\n",
      "Loaded CYP2D6.56.003: shape (4312, 7)\n",
      "Loaded CYP2D6.42.001: shape (4314, 7)\n",
      "Loaded CYP2D6.4.028: shape (4312, 7)\n",
      "Loaded CYP2D6.75.002: shape (4312, 7)\n",
      "Loaded CYP2D6.9.001: shape (4312, 7)\n",
      "Loaded CYP2D6.128.001: shape (4312, 7)\n",
      "Loaded CYP2D6.2.016: shape (4312, 7)\n",
      "Loaded CYP2D6.32.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.060: shape (4312, 7)\n",
      "Loaded CYP2D6.4.023: shape (4312, 7)\n",
      "Loaded CYP2D6.10.008: shape (4312, 7)\n",
      "Loaded CYP2D6.1.019: shape (4312, 7)\n",
      "Loaded CYP2D6.35.005: shape (4312, 7)\n",
      "Loaded CYP2D6.1.038: shape (4312, 7)\n",
      "Loaded CYP2D6.1.020: shape (4312, 7)\n",
      "Loaded CYP2D6.166.001: shape (4312, 7)\n",
      "Loaded CYP2D6.83.002: shape (4312, 7)\n",
      "Loaded CYP2D6.45.004: shape (4312, 7)\n",
      "Loaded CYP2D6.69.001: shape (4312, 7)\n",
      "Loaded CYP2D6.2.007: shape (4312, 7)\n",
      "Loaded CYP2D6.27.002: shape (4312, 7)\n",
      "Loaded CYP2D6.71.002: shape (4312, 7)\n",
      "Loaded CYP2D6.91.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.011: shape (4312, 7)\n",
      "Loaded CYP2D6.52.001: shape (4312, 7)\n",
      "Loaded CYP2D6.56.002: shape (4312, 7)\n",
      "Loaded CYP2D6.4.014: shape (4312, 7)\n",
      "Loaded CYP2D6.33.001: shape (4312, 7)\n",
      "Loaded CYP2D6.15.001: shape (4313, 7)\n",
      "Loaded CYP2D6.146.001: shape (4313, 7)\n",
      "Loaded CYP2D6.170.001: shape (4312, 7)\n",
      "Loaded CYP2D6.30.001: shape (4321, 7)\n",
      "Loaded CYP2D6.1.014: shape (4313, 7)\n",
      "Loaded CYP2D6.6.003: shape (4312, 7)\n",
      "Loaded CYP2D6.108.001: shape (4312, 7)\n",
      "Loaded CYP2D6.2.014: shape (4312, 7)\n",
      "Loaded CYP2D6.118.001: shape (4312, 7)\n",
      "Loaded CYP2D6.6.001: shape (4312, 7)\n",
      "Loaded CYP2D6.4.008: shape (4312, 7)\n",
      "Loaded CYP2D6.24.001: shape (4312, 7)\n",
      "Loaded CYP2D6.86.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.063: shape (4312, 7)\n",
      "Loaded CYP2D6.163.001: shape (4312, 7)\n",
      "Loaded CYP2D6.140.001: shape (4312, 7)\n",
      "Loaded CYP2D6.96.001: shape (4312, 7)\n",
      "Loaded CYP2D6.43.002: shape (4312, 7)\n",
      "Loaded CYP2D6.2.034: shape (4312, 7)\n",
      "Loaded CYP2D6.26.001: shape (4312, 7)\n",
      "Loaded CYP2D6.150.001: shape (4313, 7)\n",
      "Loaded CYP2D6.45.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.062: shape (4312, 7)\n",
      "Loaded CYP2D6.1.002: shape (4312, 7)\n",
      "Loaded CYP2D6.27.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.006: shape (4312, 7)\n",
      "Loaded CYP2D6.5.001: shape (4312, 7)\n",
      "Loaded CYP2D6.29.001: shape (4312, 7)\n",
      "Loaded CYP2D6.6.002: shape (4312, 7)\n",
      "Loaded CYP2D6.4.021: shape (4312, 7)\n",
      "Loaded CYP2D6.70.001: shape (4312, 7)\n",
      "Loaded CYP2D6.95.001: shape (4312, 7)\n",
      "Loaded CYP2D6.41.007: shape (4312, 7)\n",
      "Loaded CYP2D6.2.022: shape (4312, 7)\n",
      "Loaded CYP2D6.1.061: shape (4312, 7)\n",
      "Loaded CYP2D6.174.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.051: shape (4312, 7)\n",
      "Loaded CYP2D6.161.001: shape (4313, 7)\n",
      "Loaded CYP2D6.12.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.043: shape (4312, 7)\n",
      "Loaded CYP2D6.107.001: shape (4312, 7)\n",
      "Loaded CYP2D6.2.006: shape (4312, 7)\n",
      "Loaded CYP2D6.117.001: shape (4312, 7)\n",
      "Loaded CYP2D6.36.001: shape (4312, 7)\n",
      "Loaded CYP2D6.2.023: shape (4312, 7)\n",
      "Loaded CYP2D6.4.029: shape (4312, 7)\n",
      "Loaded CYP2D6.115.001: shape (4312, 7)\n",
      "Loaded CYP2D6.1.016: shape (4312, 7)\n",
      "Loaded CYP2D6.39.002: shape (4312, 7)\n",
      "Loaded CYP2D6.4.033: shape (4312, 7)\n",
      "최대 행 수: 4330\n",
      "로드된 매트릭스 개수: 361\n"
     ]
    }
   ],
   "source": [
    "#onehot 파일 폴더 경로\n",
    "onehot_folder = 'onehot7'\n",
    "onehot_files = os.listdir(onehot_folder)\n",
    "\n",
    "# 원핫인코딩 매트릭스를 저장할 딕셔너리\n",
    "onehot_matrices = {}\n",
    "\n",
    "# 최대 행 수 찾기\n",
    "max_rows = 0\n",
    "\n",
    "# 원핫인코딩 매트릭스 파일들을 로드 (헤더 없음)\n",
    "for file in onehot_files:\n",
    "    variant_name = file.split('.csv')[0].replace('_', '.')  # CYP2D6_1.001.csv -> CYP2D6.1.001\n",
    "    file_path = os.path.join(onehot_folder, file)\n",
    "    try:\n",
    "        matrix = pd.read_csv(file_path, header=None).values\n",
    "        onehot_matrices[variant_name] = matrix\n",
    "        max_rows = max(max_rows, matrix.shape[0])\n",
    "        print(f\"Loaded {variant_name}: shape {matrix.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {str(e)}\")\n",
    "\n",
    "print(f\"최대 행 수: {max_rows}\")\n",
    "print(f\"로드된 매트릭스 개수: {len(onehot_matrices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9eb9429b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape: torch.Size([2816, 8660, 7])\n",
      "Train y shape: torch.Size([2816])\n",
      "Test X shape: torch.Size([36, 8660, 7])\n",
      "Test y shape: torch.Size([36])\n"
     ]
    }
   ],
   "source": [
    "# 매트릭스 패딩 함수_원래 input으로 들어갈 X가 numpy 배열이었는데 이를 pytorch 텐서로 전환\n",
    "def pad_matrix(matrix, target_rows=4330, target_cols=7):\n",
    "    current_rows, current_cols = matrix.shape\n",
    "    padded = torch.zeros((target_rows, target_cols))\n",
    "    padded[:current_rows, :current_cols] = torch.tensor(matrix)\n",
    "    return padded\n",
    "\n",
    "# 패딩된 매트릭스로 업데이트\n",
    "for key in onehot_matrices:\n",
    "    onehot_matrices[key] = pad_matrix(onehot_matrices[key])\n",
    "\n",
    "\n",
    "# 'Variant' 컬럼의 값을 기반으로 원핫인코딩 매트릭스 매핑\n",
    "def map_variant_to_onehot(variant):\n",
    "    variants = variant.split('; ')\n",
    "    matrices = []\n",
    "    for v in variants:\n",
    "        if v in onehot_matrices:\n",
    "            matrices.append(onehot_matrices[v])\n",
    "        else:\n",
    "            print(f\"Warning: Matrix not found for variant {v}\")\n",
    "            matrices.append(torch.zeros((4330, 7)))\n",
    "    \n",
    "    # 변이가 2개 미만인 경우 0으로 채우기\n",
    "    while len(matrices) < 2:\n",
    "        matrices.append(torch.zeros((4330, 7)))\n",
    "    \n",
    "    # 첫 두 개의 매트릭스를 행 방향으로 이어붙임\n",
    "    combined_matrix = torch.vstack(matrices[:2])\n",
    "    \n",
    "    return combined_matrix\n",
    "\n",
    "# 데이터셋마다 매핑된 매트릭스를 생성하는 함수\n",
    "def map_variants_to_tensor(data):\n",
    "    mapped_matrices = []\n",
    "    for variant in data['Variant']:\n",
    "        mapped_matrix = map_variant_to_onehot(variant)\n",
    "        mapped_matrices.append(mapped_matrix)\n",
    "    \n",
    "    # 매핑된 매트릭스를 PyTorch 텐서로 변환\n",
    "    X = torch.stack(mapped_matrices)\n",
    "    y = torch.tensor(data['Class'].values)\n",
    "    return X, y\n",
    "\n",
    "# train_data에 대해 매핑된 매트릭스를 텐서로 변환\n",
    "train_X, train_y = map_variants_to_tensor(train_data)\n",
    "print(\"Train X shape:\", train_X.shape)\n",
    "print(\"Train y shape:\", train_y.shape)\n",
    "\n",
    "# test_data에 대해 매핑된 매트릭스를 텐서로 변환\n",
    "test_X, test_y = map_variants_to_tensor(test_data)\n",
    "print(\"Test X shape:\", test_X.shape)\n",
    "print(\"Test y shape:\", test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ee93a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터셋 준비\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        return torch.tensor(X, dtype=torch.float32).unsqueeze(0), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "#학습 데이터 로드    \n",
    "dataset = CustomDataset(train_X, train_y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# train 데이터를 75% 훈련, 25% 검증 데이터로 나누기\n",
    "train_size = int(0.75 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# 테스트 데이터셋 생성\n",
    "test_dataset = CustomDataset(test_X, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c032f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_probs.extend(outputs.softmax(dim=1).cpu().numpy())  # Probability for ROC-AUC\n",
    "    \n",
    "    all_labels = np.array(all_labels)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = (all_labels == all_predictions).mean()\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_labels, all_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eba292af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(20, 7), stride=(1, 1))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 1))  # 추가된 풀링 층\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(20, 1), stride=(1, 1))\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 1))  # 추가된 풀링 층\n",
    "        \n",
    "        # 풀링 층 추가로 인한 출력 크기 변경을 고려한 새로운 계산\n",
    "        self.fc1 = nn.Linear(16 * ((((8660 - 20) // 2) - 20) // 2), 128)\n",
    "        self.fc2 = nn.Linear(128, 3)  # 출력 층\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.pool1(x)  # 풀링 적용\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.pool2(x)  # 풀링 적용\n",
    "        x = x.view(x.size(0), -1)  # 평탄화\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4d811d",
   "metadata": {},
   "source": [
    "### AUPR 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0da26b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 1.0418, Train Acc: 0.4242\n",
      "Val Loss: 0.9305, Val Acc: 0.6596\n",
      "----------------------------------------\n",
      "Epoch 2/30\n",
      "Train Loss: 0.7491, Train Acc: 0.6492\n",
      "Val Loss: 0.5323, Val Acc: 0.7624\n",
      "----------------------------------------\n",
      "Epoch 3/30\n",
      "Train Loss: 0.4869, Train Acc: 0.7912\n",
      "Val Loss: 0.6436, Val Acc: 0.6879\n",
      "----------------------------------------\n",
      "Epoch 4/30\n",
      "Train Loss: 0.4128, Train Acc: 0.8181\n",
      "Val Loss: 0.3432, Val Acc: 0.8440\n",
      "----------------------------------------\n",
      "Epoch 5/30\n",
      "Train Loss: 0.3608, Train Acc: 0.8390\n",
      "Val Loss: 0.3577, Val Acc: 0.8369\n",
      "----------------------------------------\n",
      "Epoch 6/30\n",
      "Train Loss: 0.3272, Train Acc: 0.8579\n",
      "Val Loss: 0.3184, Val Acc: 0.8617\n",
      "----------------------------------------\n",
      "Epoch 7/30\n",
      "Train Loss: 0.3100, Train Acc: 0.8579\n",
      "Val Loss: 0.3134, Val Acc: 0.8582\n",
      "----------------------------------------\n",
      "Epoch 8/30\n",
      "Train Loss: 0.2990, Train Acc: 0.8607\n",
      "Val Loss: 0.3267, Val Acc: 0.8652\n",
      "----------------------------------------\n",
      "Epoch 9/30\n",
      "Train Loss: 0.2912, Train Acc: 0.8646\n",
      "Val Loss: 0.3422, Val Acc: 0.8582\n",
      "----------------------------------------\n",
      "Epoch 10/30\n",
      "Train Loss: 0.2836, Train Acc: 0.8702\n",
      "Val Loss: 0.3805, Val Acc: 0.8298\n",
      "----------------------------------------\n",
      "Early stopping 적용.\n",
      "Fold 1 Validation AUC: 0.9625, AUPR: 0.9340\n",
      "Fold 1 Precision: 0.8616, Recall: 0.8622, F1 score: 0.8619\n",
      "\n",
      "Fold 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 1.1133, Train Acc: 0.4088\n",
      "Val Loss: 1.0386, Val Acc: 0.5284\n",
      "----------------------------------------\n",
      "Epoch 2/30\n",
      "Train Loss: 0.9559, Train Acc: 0.5134\n",
      "Val Loss: 0.7815, Val Acc: 0.6454\n",
      "----------------------------------------\n",
      "Epoch 3/30\n",
      "Train Loss: 0.6222, Train Acc: 0.7372\n",
      "Val Loss: 0.5348, Val Acc: 0.7589\n",
      "----------------------------------------\n",
      "Epoch 4/30\n",
      "Train Loss: 0.4511, Train Acc: 0.8031\n",
      "Val Loss: 0.4335, Val Acc: 0.8227\n",
      "----------------------------------------\n",
      "Epoch 5/30\n",
      "Train Loss: 0.3749, Train Acc: 0.8327\n",
      "Val Loss: 0.4269, Val Acc: 0.7837\n",
      "----------------------------------------\n",
      "Epoch 6/30\n",
      "Train Loss: 0.3553, Train Acc: 0.8374\n",
      "Val Loss: 0.3936, Val Acc: 0.8227\n",
      "----------------------------------------\n",
      "Epoch 7/30\n",
      "Train Loss: 0.3290, Train Acc: 0.8512\n",
      "Val Loss: 0.3537, Val Acc: 0.8475\n",
      "----------------------------------------\n",
      "Epoch 8/30\n",
      "Train Loss: 0.3212, Train Acc: 0.8528\n",
      "Val Loss: 0.3468, Val Acc: 0.8404\n",
      "----------------------------------------\n",
      "Epoch 9/30\n",
      "Train Loss: 0.3005, Train Acc: 0.8642\n",
      "Val Loss: 0.3340, Val Acc: 0.8369\n",
      "----------------------------------------\n",
      "Epoch 10/30\n",
      "Train Loss: 0.2860, Train Acc: 0.8725\n",
      "Val Loss: 0.3316, Val Acc: 0.8511\n",
      "----------------------------------------\n",
      "Epoch 11/30\n",
      "Train Loss: 0.2828, Train Acc: 0.8674\n",
      "Val Loss: 0.3350, Val Acc: 0.8369\n",
      "----------------------------------------\n",
      "Epoch 12/30\n",
      "Train Loss: 0.2681, Train Acc: 0.8812\n",
      "Val Loss: 0.3361, Val Acc: 0.8440\n",
      "----------------------------------------\n",
      "Epoch 13/30\n",
      "Train Loss: 0.2609, Train Acc: 0.8820\n",
      "Val Loss: 0.3469, Val Acc: 0.8511\n",
      "----------------------------------------\n",
      "Early stopping 적용.\n",
      "Fold 2 Validation AUC: 0.9630, AUPR: 0.9335\n",
      "Fold 2 Precision: 0.8608, Recall: 0.8531, F1 score: 0.8565\n",
      "\n",
      "Fold 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 1.0605, Train Acc: 0.4250\n",
      "Val Loss: 1.0504, Val Acc: 0.4929\n",
      "----------------------------------------\n",
      "Epoch 2/30\n",
      "Train Loss: 0.8672, Train Acc: 0.5900\n",
      "Val Loss: 0.7198, Val Acc: 0.6950\n",
      "----------------------------------------\n",
      "Epoch 3/30\n",
      "Train Loss: 0.5864, Train Acc: 0.7380\n",
      "Val Loss: 0.5560, Val Acc: 0.6950\n",
      "----------------------------------------\n",
      "Epoch 4/30\n",
      "Train Loss: 0.4341, Train Acc: 0.8094\n",
      "Val Loss: 0.3836, Val Acc: 0.8404\n",
      "----------------------------------------\n",
      "Epoch 5/30\n",
      "Train Loss: 0.3663, Train Acc: 0.8418\n",
      "Val Loss: 0.3254, Val Acc: 0.8475\n",
      "----------------------------------------\n",
      "Epoch 6/30\n",
      "Train Loss: 0.3273, Train Acc: 0.8567\n",
      "Val Loss: 0.3182, Val Acc: 0.8440\n",
      "----------------------------------------\n",
      "Epoch 7/30\n",
      "Train Loss: 0.3235, Train Acc: 0.8552\n",
      "Val Loss: 0.2952, Val Acc: 0.8617\n",
      "----------------------------------------\n",
      "Epoch 8/30\n",
      "Train Loss: 0.3095, Train Acc: 0.8575\n",
      "Val Loss: 0.2999, Val Acc: 0.8759\n",
      "----------------------------------------\n",
      "Epoch 9/30\n",
      "Train Loss: 0.2934, Train Acc: 0.8662\n",
      "Val Loss: 0.3091, Val Acc: 0.8759\n",
      "----------------------------------------\n",
      "Epoch 10/30\n",
      "Train Loss: 0.2894, Train Acc: 0.8741\n",
      "Val Loss: 0.2848, Val Acc: 0.8759\n",
      "----------------------------------------\n",
      "Epoch 11/30\n",
      "Train Loss: 0.2771, Train Acc: 0.8733\n",
      "Val Loss: 0.2962, Val Acc: 0.8617\n",
      "----------------------------------------\n",
      "Epoch 12/30\n",
      "Train Loss: 0.2754, Train Acc: 0.8753\n",
      "Val Loss: 0.3265, Val Acc: 0.8546\n",
      "----------------------------------------\n",
      "Epoch 13/30\n",
      "Train Loss: 0.2650, Train Acc: 0.8832\n",
      "Val Loss: 0.2799, Val Acc: 0.8723\n",
      "----------------------------------------\n",
      "Epoch 14/30\n",
      "Train Loss: 0.2540, Train Acc: 0.8879\n",
      "Val Loss: 0.2924, Val Acc: 0.8617\n",
      "----------------------------------------\n",
      "Epoch 15/30\n",
      "Train Loss: 0.2467, Train Acc: 0.8903\n",
      "Val Loss: 0.2877, Val Acc: 0.8688\n",
      "----------------------------------------\n",
      "Epoch 16/30\n",
      "Train Loss: 0.2343, Train Acc: 0.8958\n",
      "Val Loss: 0.3019, Val Acc: 0.8723\n",
      "----------------------------------------\n",
      "Early stopping 적용.\n",
      "Fold 3 Validation AUC: 0.9711, AUPR: 0.9498\n",
      "Fold 3 Precision: 0.8784, Recall: 0.8731, F1 score: 0.8744\n",
      "\n",
      "Fold 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 1.0920, Train Acc: 0.3856\n",
      "Val Loss: 0.9986, Val Acc: 0.4291\n",
      "----------------------------------------\n",
      "Epoch 2/30\n",
      "Train Loss: 0.9443, Train Acc: 0.5391\n",
      "Val Loss: 0.7608, Val Acc: 0.7376\n",
      "----------------------------------------\n",
      "Epoch 3/30\n",
      "Train Loss: 0.6582, Train Acc: 0.7131\n",
      "Val Loss: 0.5025, Val Acc: 0.7908\n",
      "----------------------------------------\n",
      "Epoch 4/30\n",
      "Train Loss: 0.4664, Train Acc: 0.7999\n",
      "Val Loss: 0.3547, Val Acc: 0.8582\n",
      "----------------------------------------\n",
      "Epoch 5/30\n",
      "Train Loss: 0.3585, Train Acc: 0.8350\n",
      "Val Loss: 0.3451, Val Acc: 0.8475\n",
      "----------------------------------------\n",
      "Epoch 6/30\n",
      "Train Loss: 0.3328, Train Acc: 0.8457\n",
      "Val Loss: 0.3158, Val Acc: 0.8617\n",
      "----------------------------------------\n",
      "Epoch 7/30\n",
      "Train Loss: 0.3027, Train Acc: 0.8658\n",
      "Val Loss: 0.3059, Val Acc: 0.8688\n",
      "----------------------------------------\n",
      "Epoch 8/30\n",
      "Train Loss: 0.2900, Train Acc: 0.8694\n",
      "Val Loss: 0.3374, Val Acc: 0.8262\n",
      "----------------------------------------\n",
      "Epoch 9/30\n",
      "Train Loss: 0.2861, Train Acc: 0.8710\n",
      "Val Loss: 0.3159, Val Acc: 0.8546\n",
      "----------------------------------------\n",
      "Epoch 10/30\n",
      "Train Loss: 0.2732, Train Acc: 0.8800\n",
      "Val Loss: 0.3540, Val Acc: 0.8404\n",
      "----------------------------------------\n",
      "Early stopping 적용.\n",
      "Fold 4 Validation AUC: 0.9701, AUPR: 0.9491\n",
      "Fold 4 Precision: 0.8733, Recall: 0.8722, F1 score: 0.8727\n",
      "\n",
      "Fold 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 1.0698, Train Acc: 0.3911\n",
      "Val Loss: 1.0408, Val Acc: 0.3830\n",
      "----------------------------------------\n",
      "Epoch 2/30\n",
      "Train Loss: 0.9194, Train Acc: 0.5438\n",
      "Val Loss: 0.6765, Val Acc: 0.7270\n",
      "----------------------------------------\n",
      "Epoch 3/30\n",
      "Train Loss: 0.5802, Train Acc: 0.7439\n",
      "Val Loss: 0.4335, Val Acc: 0.8369\n",
      "----------------------------------------\n",
      "Epoch 4/30\n",
      "Train Loss: 0.4128, Train Acc: 0.8129\n",
      "Val Loss: 0.4365, Val Acc: 0.8617\n",
      "----------------------------------------\n",
      "Epoch 5/30\n",
      "Train Loss: 0.3698, Train Acc: 0.8425\n",
      "Val Loss: 0.3141, Val Acc: 0.8794\n",
      "----------------------------------------\n",
      "Epoch 6/30\n",
      "Train Loss: 0.3222, Train Acc: 0.8575\n",
      "Val Loss: 0.3207, Val Acc: 0.8688\n",
      "----------------------------------------\n",
      "Epoch 7/30\n",
      "Train Loss: 0.3227, Train Acc: 0.8556\n",
      "Val Loss: 0.3171, Val Acc: 0.8546\n",
      "----------------------------------------\n",
      "Epoch 8/30\n",
      "Train Loss: 0.3117, Train Acc: 0.8619\n",
      "Val Loss: 0.2918, Val Acc: 0.9007\n",
      "----------------------------------------\n",
      "Epoch 9/30\n",
      "Train Loss: 0.2956, Train Acc: 0.8674\n",
      "Val Loss: 0.3208, Val Acc: 0.8723\n",
      "----------------------------------------\n",
      "Epoch 10/30\n",
      "Train Loss: 0.2885, Train Acc: 0.8694\n",
      "Val Loss: 0.3576, Val Acc: 0.8511\n",
      "----------------------------------------\n",
      "Epoch 11/30\n",
      "Train Loss: 0.2748, Train Acc: 0.8702\n",
      "Val Loss: 0.2827, Val Acc: 0.8901\n",
      "----------------------------------------\n",
      "Epoch 12/30\n",
      "Train Loss: 0.2706, Train Acc: 0.8808\n",
      "Val Loss: 0.3216, Val Acc: 0.8830\n",
      "----------------------------------------\n",
      "Epoch 13/30\n",
      "Train Loss: 0.2634, Train Acc: 0.8804\n",
      "Val Loss: 0.3045, Val Acc: 0.8759\n",
      "----------------------------------------\n",
      "Epoch 14/30\n",
      "Train Loss: 0.2537, Train Acc: 0.8848\n",
      "Val Loss: 0.2912, Val Acc: 0.8865\n",
      "----------------------------------------\n",
      "Early stopping 적용.\n",
      "Fold 5 Validation AUC: 0.9745, AUPR: 0.9510\n",
      "Fold 5 Precision: 0.8936, Recall: 0.8929, F1 score: 0.8932\n",
      "\n",
      "Fold 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 1.0596, Train Acc: 0.4029\n",
      "Val Loss: 1.0260, Val Acc: 0.4255\n",
      "----------------------------------------\n",
      "Epoch 2/30\n",
      "Train Loss: 0.9947, Train Acc: 0.4858\n",
      "Val Loss: 0.8672, Val Acc: 0.5390\n",
      "----------------------------------------\n",
      "Epoch 3/30\n",
      "Train Loss: 0.6980, Train Acc: 0.6835\n",
      "Val Loss: 0.4734, Val Acc: 0.8121\n",
      "----------------------------------------\n",
      "Epoch 4/30\n",
      "Train Loss: 0.4546, Train Acc: 0.7995\n",
      "Val Loss: 0.3630, Val Acc: 0.8546\n",
      "----------------------------------------\n",
      "Epoch 5/30\n",
      "Train Loss: 0.3740, Train Acc: 0.8327\n",
      "Val Loss: 0.3309, Val Acc: 0.8262\n",
      "----------------------------------------\n",
      "Epoch 6/30\n",
      "Train Loss: 0.3496, Train Acc: 0.8414\n",
      "Val Loss: 0.3120, Val Acc: 0.8404\n",
      "----------------------------------------\n",
      "Epoch 7/30\n",
      "Train Loss: 0.3259, Train Acc: 0.8508\n",
      "Val Loss: 0.3154, Val Acc: 0.8617\n",
      "----------------------------------------\n",
      "Epoch 8/30\n",
      "Train Loss: 0.3131, Train Acc: 0.8567\n",
      "Val Loss: 0.2942, Val Acc: 0.8759\n",
      "----------------------------------------\n",
      "Epoch 9/30\n",
      "Train Loss: 0.3036, Train Acc: 0.8713\n",
      "Val Loss: 0.2969, Val Acc: 0.8652\n",
      "----------------------------------------\n",
      "Epoch 10/30\n",
      "Train Loss: 0.2894, Train Acc: 0.8642\n",
      "Val Loss: 0.3234, Val Acc: 0.8546\n",
      "----------------------------------------\n",
      "Epoch 11/30\n",
      "Train Loss: 0.2908, Train Acc: 0.8686\n",
      "Val Loss: 0.3048, Val Acc: 0.8759\n",
      "----------------------------------------\n",
      "Early stopping 적용.\n",
      "Fold 6 Validation AUC: 0.9676, AUPR: 0.9433\n",
      "Fold 6 Precision: 0.8817, Recall: 0.8820, F1 score: 0.8803\n",
      "\n",
      "Fold 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 1.0766, Train Acc: 0.3783\n",
      "Val Loss: 1.0080, Val Acc: 0.4164\n",
      "----------------------------------------\n",
      "Epoch 2/30\n",
      "Train Loss: 0.9249, Train Acc: 0.5400\n",
      "Val Loss: 0.7097, Val Acc: 0.7260\n",
      "----------------------------------------\n",
      "Epoch 3/30\n",
      "Train Loss: 0.6099, Train Acc: 0.7199\n",
      "Val Loss: 0.4919, Val Acc: 0.7829\n",
      "----------------------------------------\n",
      "Epoch 4/30\n",
      "Train Loss: 0.4396, Train Acc: 0.8075\n",
      "Val Loss: 0.4245, Val Acc: 0.8256\n",
      "----------------------------------------\n",
      "Epoch 5/30\n",
      "Train Loss: 0.3540, Train Acc: 0.8469\n",
      "Val Loss: 0.3688, Val Acc: 0.8363\n",
      "----------------------------------------\n",
      "Epoch 6/30\n",
      "Train Loss: 0.3233, Train Acc: 0.8576\n",
      "Val Loss: 0.3705, Val Acc: 0.8434\n",
      "----------------------------------------\n",
      "Epoch 7/30\n",
      "Train Loss: 0.3180, Train Acc: 0.8568\n",
      "Val Loss: 0.3602, Val Acc: 0.8363\n",
      "----------------------------------------\n",
      "Epoch 8/30\n",
      "Train Loss: 0.3034, Train Acc: 0.8611\n",
      "Val Loss: 0.3740, Val Acc: 0.8434\n",
      "----------------------------------------\n",
      "Epoch 9/30\n",
      "Train Loss: 0.2829, Train Acc: 0.8742\n",
      "Val Loss: 0.3534, Val Acc: 0.8434\n",
      "----------------------------------------\n",
      "Epoch 10/30\n",
      "Train Loss: 0.2752, Train Acc: 0.8667\n",
      "Val Loss: 0.3424, Val Acc: 0.8648\n",
      "----------------------------------------\n",
      "Epoch 11/30\n",
      "Train Loss: 0.2666, Train Acc: 0.8817\n",
      "Val Loss: 0.3639, Val Acc: 0.8470\n",
      "----------------------------------------\n",
      "Epoch 12/30\n",
      "Train Loss: 0.2765, Train Acc: 0.8797\n",
      "Val Loss: 0.3420, Val Acc: 0.8577\n",
      "----------------------------------------\n",
      "Epoch 13/30\n",
      "Train Loss: 0.2784, Train Acc: 0.8789\n",
      "Val Loss: 0.3678, Val Acc: 0.8256\n",
      "----------------------------------------\n",
      "Epoch 14/30\n",
      "Train Loss: 0.2703, Train Acc: 0.8797\n",
      "Val Loss: 0.3546, Val Acc: 0.8505\n",
      "----------------------------------------\n",
      "Epoch 15/30\n",
      "Train Loss: 0.2501, Train Acc: 0.8864\n",
      "Val Loss: 0.3564, Val Acc: 0.8541\n",
      "----------------------------------------\n",
      "Early stopping 적용.\n",
      "Fold 7 Validation AUC: 0.9586, AUPR: 0.9211\n",
      "Fold 7 Precision: 0.8638, Recall: 0.8614, F1 score: 0.8622\n",
      "\n",
      "Fold 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 1.0993, Train Acc: 0.4118\n",
      "Val Loss: 1.0002, Val Acc: 0.4591\n",
      "----------------------------------------\n",
      "Epoch 2/30\n",
      "Train Loss: 0.8480, Train Acc: 0.5929\n",
      "Val Loss: 0.7097, Val Acc: 0.6690\n",
      "----------------------------------------\n",
      "Epoch 3/30\n",
      "Train Loss: 0.5516, Train Acc: 0.7487\n",
      "Val Loss: 0.4829, Val Acc: 0.7865\n",
      "----------------------------------------\n",
      "Epoch 4/30\n",
      "Train Loss: 0.4127, Train Acc: 0.8233\n",
      "Val Loss: 0.4084, Val Acc: 0.8327\n",
      "----------------------------------------\n",
      "Epoch 5/30\n",
      "Train Loss: 0.3695, Train Acc: 0.8292\n",
      "Val Loss: 0.3612, Val Acc: 0.8434\n",
      "----------------------------------------\n",
      "Epoch 6/30\n",
      "Train Loss: 0.3343, Train Acc: 0.8536\n",
      "Val Loss: 0.3457, Val Acc: 0.8505\n",
      "----------------------------------------\n",
      "Epoch 7/30\n",
      "Train Loss: 0.3082, Train Acc: 0.8647\n",
      "Val Loss: 0.3508, Val Acc: 0.8683\n",
      "----------------------------------------\n",
      "Epoch 8/30\n",
      "Train Loss: 0.3050, Train Acc: 0.8556\n",
      "Val Loss: 0.3201, Val Acc: 0.8399\n",
      "----------------------------------------\n",
      "Epoch 9/30\n",
      "Train Loss: 0.2990, Train Acc: 0.8710\n",
      "Val Loss: 0.3250, Val Acc: 0.8719\n",
      "----------------------------------------\n",
      "Epoch 10/30\n",
      "Train Loss: 0.2861, Train Acc: 0.8714\n",
      "Val Loss: 0.3000, Val Acc: 0.8399\n",
      "----------------------------------------\n",
      "Epoch 11/30\n",
      "Train Loss: 0.2791, Train Acc: 0.8686\n",
      "Val Loss: 0.3288, Val Acc: 0.8541\n",
      "----------------------------------------\n",
      "Epoch 12/30\n",
      "Train Loss: 0.2683, Train Acc: 0.8757\n",
      "Val Loss: 0.3114, Val Acc: 0.8577\n",
      "----------------------------------------\n",
      "Epoch 13/30\n",
      "Train Loss: 0.2663, Train Acc: 0.8832\n",
      "Val Loss: 0.3234, Val Acc: 0.8434\n",
      "----------------------------------------\n",
      "Early stopping 적용.\n",
      "Fold 8 Validation AUC: 0.9671, AUPR: 0.9430\n",
      "Fold 8 Precision: 0.8462, Recall: 0.8430, F1 score: 0.8445\n",
      "\n",
      "Fold 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 1.0708, Train Acc: 0.4233\n",
      "Val Loss: 1.0279, Val Acc: 0.4093\n",
      "----------------------------------------\n",
      "Epoch 2/30\n",
      "Train Loss: 0.9267, Train Acc: 0.5487\n",
      "Val Loss: 0.7603, Val Acc: 0.6655\n",
      "----------------------------------------\n",
      "Epoch 3/30\n",
      "Train Loss: 0.6325, Train Acc: 0.7156\n",
      "Val Loss: 0.5770, Val Acc: 0.7367\n",
      "----------------------------------------\n",
      "Epoch 4/30\n",
      "Train Loss: 0.4608, Train Acc: 0.7905\n",
      "Val Loss: 0.4740, Val Acc: 0.7936\n",
      "----------------------------------------\n",
      "Epoch 5/30\n",
      "Train Loss: 0.3762, Train Acc: 0.8406\n",
      "Val Loss: 0.4404, Val Acc: 0.7829\n",
      "----------------------------------------\n",
      "Epoch 6/30\n",
      "Train Loss: 0.3407, Train Acc: 0.8568\n",
      "Val Loss: 0.4047, Val Acc: 0.8043\n",
      "----------------------------------------\n",
      "Epoch 7/30\n",
      "Train Loss: 0.3274, Train Acc: 0.8584\n",
      "Val Loss: 0.4125, Val Acc: 0.7972\n",
      "----------------------------------------\n",
      "Epoch 8/30\n",
      "Train Loss: 0.3010, Train Acc: 0.8722\n",
      "Val Loss: 0.4035, Val Acc: 0.8185\n",
      "----------------------------------------\n",
      "Epoch 9/30\n",
      "Train Loss: 0.3006, Train Acc: 0.8682\n",
      "Val Loss: 0.3918, Val Acc: 0.8078\n",
      "----------------------------------------\n",
      "Epoch 10/30\n",
      "Train Loss: 0.2833, Train Acc: 0.8734\n",
      "Val Loss: 0.3849, Val Acc: 0.8043\n",
      "----------------------------------------\n",
      "Epoch 11/30\n",
      "Train Loss: 0.2805, Train Acc: 0.8734\n",
      "Val Loss: 0.3758, Val Acc: 0.8114\n",
      "----------------------------------------\n",
      "Epoch 12/30\n",
      "Train Loss: 0.2755, Train Acc: 0.8773\n",
      "Val Loss: 0.4324, Val Acc: 0.8185\n",
      "----------------------------------------\n",
      "Epoch 13/30\n",
      "Train Loss: 0.2542, Train Acc: 0.8856\n",
      "Val Loss: 0.4141, Val Acc: 0.8292\n",
      "----------------------------------------\n",
      "Epoch 14/30\n",
      "Train Loss: 0.2580, Train Acc: 0.8801\n",
      "Val Loss: 0.3925, Val Acc: 0.8221\n",
      "----------------------------------------\n",
      "Early stopping 적용.\n",
      "Fold 9 Validation AUC: 0.9491, AUPR: 0.9073\n",
      "Fold 9 Precision: 0.8168, Recall: 0.8175, F1 score: 0.8170\n",
      "\n",
      "Fold 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 1.0828, Train Acc: 0.4095\n",
      "Val Loss: 1.0405, Val Acc: 0.3915\n",
      "----------------------------------------\n",
      "Epoch 2/30\n",
      "Train Loss: 0.9248, Train Acc: 0.5365\n",
      "Val Loss: 0.7969, Val Acc: 0.6228\n",
      "----------------------------------------\n",
      "Epoch 3/30\n",
      "Train Loss: 0.5875, Train Acc: 0.7337\n",
      "Val Loss: 0.5402, Val Acc: 0.7580\n",
      "----------------------------------------\n",
      "Epoch 4/30\n",
      "Train Loss: 0.4079, Train Acc: 0.8170\n",
      "Val Loss: 0.4641, Val Acc: 0.7900\n",
      "----------------------------------------\n",
      "Epoch 5/30\n",
      "Train Loss: 0.3626, Train Acc: 0.8367\n",
      "Val Loss: 0.4149, Val Acc: 0.8114\n",
      "----------------------------------------\n",
      "Epoch 6/30\n",
      "Train Loss: 0.3282, Train Acc: 0.8469\n",
      "Val Loss: 0.3728, Val Acc: 0.8292\n",
      "----------------------------------------\n",
      "Epoch 7/30\n",
      "Train Loss: 0.3005, Train Acc: 0.8651\n",
      "Val Loss: 0.4103, Val Acc: 0.8078\n",
      "----------------------------------------\n",
      "Epoch 8/30\n",
      "Train Loss: 0.2954, Train Acc: 0.8698\n",
      "Val Loss: 0.3628, Val Acc: 0.8434\n",
      "----------------------------------------\n",
      "Epoch 9/30\n",
      "Train Loss: 0.2791, Train Acc: 0.8742\n",
      "Val Loss: 0.3871, Val Acc: 0.8114\n",
      "----------------------------------------\n",
      "Epoch 10/30\n",
      "Train Loss: 0.2851, Train Acc: 0.8738\n",
      "Val Loss: 0.3817, Val Acc: 0.8327\n",
      "----------------------------------------\n",
      "Epoch 11/30\n",
      "Train Loss: 0.2687, Train Acc: 0.8785\n",
      "Val Loss: 0.3440, Val Acc: 0.8470\n",
      "----------------------------------------\n",
      "Epoch 12/30\n",
      "Train Loss: 0.2625, Train Acc: 0.8828\n",
      "Val Loss: 0.3872, Val Acc: 0.8363\n",
      "----------------------------------------\n",
      "Epoch 13/30\n",
      "Train Loss: 0.2556, Train Acc: 0.8809\n",
      "Val Loss: 0.3825, Val Acc: 0.8470\n",
      "----------------------------------------\n",
      "Epoch 14/30\n",
      "Train Loss: 0.2522, Train Acc: 0.8864\n",
      "Val Loss: 0.4639, Val Acc: 0.8149\n",
      "----------------------------------------\n",
      "Early stopping 적용.\n",
      "Fold 10 Validation AUC: 0.9603, AUPR: 0.9227\n",
      "Fold 10 Precision: 0.8470, Recall: 0.8545, F1 score: 0.8497\n",
      "\n",
      "Average Validation AUC: 0.9644, AUPR: 0.9355\n",
      "Average Validation Precision: 0.8623, Recall: 0.8612, F1 score: 0.8612\n"
     ]
    }
   ],
   "source": [
    "# Stratified 10-fold 교차 검증을 위해 데이터를 k개의 폴드로 나눔\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# 결과 저장용 리스트 초기화\n",
    "fold_train_losses, fold_val_losses = [], []\n",
    "fold_train_accs, fold_val_accs = [], []\n",
    "fold_val_aucs, fold_val_precisions, fold_val_recalls, fold_val_f1s = [], [], [], []\n",
    "fold_val_auprs = []  # AUPR 값을 저장할 리스트를 추가로 초기화\n",
    "\n",
    "# 전체 데이터로 모델 학습 및 성능 평가\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_X, train_y)):\n",
    "    print(f\"\\nFold {fold+1}/{n_splits}\")\n",
    "    \n",
    "    # Train과 validation 데이터를 폴드별로 나눔\n",
    "    X_train_fold, X_val_fold = train_X[train_idx], train_X[val_idx]\n",
    "    y_train_fold, y_val_fold = train_y[train_idx], train_y[val_idx]\n",
    "    \n",
    "    # 커스텀 데이터셋 생성\n",
    "    train_dataset_fold = CustomDataset(X_train_fold, y_train_fold)\n",
    "    val_dataset_fold = CustomDataset(X_val_fold, y_val_fold)\n",
    "    \n",
    "    # DataLoader 생성\n",
    "    train_loader_fold = DataLoader(train_dataset_fold, batch_size=32, shuffle=True)\n",
    "    val_loader_fold = DataLoader(val_dataset_fold, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # 모델 초기화 (매 fold마다 모델을 새로 초기화)\n",
    "    model = SimpleCNN().to(device)   # 예시로 CNN 모델을 사용했습니다. 본인의 모델로 변경 가능\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device) \n",
    "    \n",
    "    # Early stopping을 위한 변수 초기화\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "\n",
    "    # 각 fold에서 epoch마다 학습\n",
    "    num_epochs = 30\n",
    "    for epoch in range(num_epochs):\n",
    "        # 1 epoch 학습\n",
    "        train_loss, train_acc = train_epoch(model, train_loader_fold, criterion, optimizer, device)\n",
    "        \n",
    "        # Validation 평가\n",
    "        val_loss, val_acc, val_labels, val_probs = evaluate(model, val_loader_fold, criterion, device)\n",
    "        \n",
    "        # 성능 저장\n",
    "        fold_train_losses.append(train_loss)\n",
    "        fold_val_losses.append(val_loss)\n",
    "        fold_train_accs.append(train_acc)\n",
    "        fold_val_accs.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Early Stopping 체크\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # 모델 저장\n",
    "            torch.save(model.state_dict(), f'CNN_best_model_fold_{fold+1}.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping 적용.\")\n",
    "                break\n",
    "\n",
    "    # Fold별 Validation 성능 평가\n",
    "    model.load_state_dict(torch.load(f'CNN_best_model_fold_{fold+1}.pth'))\n",
    "    val_loss, val_acc, val_labels, val_probs = evaluate(model, val_loader_fold, criterion, device)\n",
    "\n",
    "    # AUC-ROC 계산 및 성능 기록\n",
    "    val_auc = roc_auc_score(val_labels, val_probs, multi_class='ovr')\n",
    "    fold_val_aucs.append(val_auc)\n",
    "\n",
    "    val_precision = precision_score(val_labels, val_probs.argmax(axis=1), average='macro')\n",
    "    val_recall = recall_score(val_labels, val_probs.argmax(axis=1), average='macro')\n",
    "    val_f1 = f1_score(val_labels, val_probs.argmax(axis=1), average='macro')\n",
    "\n",
    "    fold_val_precisions.append(val_precision)\n",
    "    fold_val_recalls.append(val_recall)\n",
    "    fold_val_f1s.append(val_f1)\n",
    "\n",
    "    # AUPR 계산 (multi-class one-vs-rest 방식)\n",
    "    val_aupr = []\n",
    "    for i in range(val_probs.shape[1]):\n",
    "        aupr_class = average_precision_score((val_labels == i).astype(int), val_probs[:, i])\n",
    "        val_aupr.append(aupr_class)\n",
    "\n",
    "    # 각 클래스에 대한 AUPR 기록\n",
    "    fold_val_auprs.append(np.mean(val_aupr))  # 평균 AUPR 계산\n",
    "    print(f\"Fold {fold+1} Validation AUC: {val_auc:.4f}, AUPR: {np.mean(val_aupr):.4f}\")\n",
    "    print(f\"Fold {fold+1} Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1 score: {val_f1:.4f}\")\n",
    "\n",
    "# 교차 검증 성능 평균 계산 (AUPR 추가)\n",
    "avg_val_auc = sum(fold_val_aucs) / n_splits\n",
    "avg_val_aupr = sum(fold_val_auprs) / n_splits\n",
    "avg_val_precision = sum(fold_val_precisions) / n_splits\n",
    "avg_val_recall = sum(fold_val_recalls) / n_splits\n",
    "avg_val_f1 = sum(fold_val_f1s) / n_splits\n",
    "\n",
    "print(f\"\\nAverage Validation AUC: {avg_val_auc:.4f}, AUPR: {avg_val_aupr:.4f}\")\n",
    "print(f\"Average Validation Precision: {avg_val_precision:.4f}, Recall: {avg_val_recall:.4f}, F1 score: {avg_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20d59c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 1...\n",
      "Evaluating fold 2...\n",
      "Evaluating fold 3...\n",
      "Evaluating fold 4...\n",
      "Evaluating fold 5...\n",
      "Evaluating fold 6...\n",
      "Evaluating fold 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n",
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n",
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n",
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n",
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n",
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n",
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 8...\n",
      "Evaluating fold 9...\n",
      "Evaluating fold 10...\n",
      "Overall Metrics Across 10 Folds:\n",
      "Accuracy: 0.8278 ± 0.0408\n",
      "AUC: 0.9520 ± 0.0100\n",
      "AUPR: 0.9046 ± 0.0100\n",
      "Precision: 0.7520 ± 0.0485\n",
      "Recall: 0.8784 ± 0.0288\n",
      "F1 Score: 0.7817 ± 0.0503\n",
      "\n",
      "Class 0 Metrics Across 10 Folds:\n",
      "  Accuracy: 0.8556 ± 0.0408\n",
      "  AUC: 0.9365 ± 0.0217\n",
      "  AUPR: 0.9345 ± 0.0203\n",
      "  Precision: 0.8358 ± 0.0334\n",
      "  Recall: 0.8647 ± 0.0791\n",
      "  F1 Score: 0.8482 ± 0.0473\n",
      "\n",
      "Class 1 Metrics Across 10 Folds:\n",
      "  Accuracy: 0.9389 ± 0.0242\n",
      "  AUC: 0.9853 ± 0.0000\n",
      "  AUPR: 0.8333 ± 0.0000\n",
      "  Precision: 0.4967 ± 0.1016\n",
      "  Recall: 1.0000 ± 0.0000\n",
      "  F1 Score: 0.6576 ± 0.0898\n",
      "\n",
      "Class 2 Metrics Across 10 Folds:\n",
      "  Accuracy: 0.8611 ± 0.0248\n",
      "  AUC: 0.9341 ± 0.0116\n",
      "  AUPR: 0.9461 ± 0.0109\n",
      "  Precision: 0.9235 ± 0.0341\n",
      "  Recall: 0.7706 ± 0.0412\n",
      "  F1 Score: 0.8394 ± 0.0304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n",
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n",
      "/data/home/bmi-lab/anaconda3/envs/nayb37/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 성능 저장을 위한 리스트 초기화\n",
    "fold_test_losses = []\n",
    "fold_accuracies = []\n",
    "fold_aucs = []\n",
    "fold_auprs = []\n",
    "fold_precisions = []\n",
    "fold_recalls = []\n",
    "fold_f1_scores = []\n",
    "\n",
    "# 클래스별 성능 저장용 딕셔너리 초기화\n",
    "class_metrics = {}\n",
    "\n",
    "for fold in range(10):\n",
    "    print(f\"Evaluating fold {fold + 1}...\")\n",
    "    \n",
    "    # 모델 로드 및 평가 모드로 전환\n",
    "    model.load_state_dict(torch.load(f'CNN_best_model_fold_{fold+1}.pth'))\n",
    "    model.eval()\n",
    "    \n",
    "    # 테스트 세트 평가\n",
    "    test_loss, test_acc, test_labels, test_probs = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    # num_classes 동적으로 설정\n",
    "    num_classes = test_probs.shape[1]\n",
    "\n",
    "    # 클래스별 메트릭 저장용 딕셔너리 초기화\n",
    "    if fold == 0:\n",
    "        class_metrics = {\n",
    "            i: {\"accuracy\": [], \"auc\": [], \"aupr\": [], \"precision\": [], \"recall\": [], \"f1\": []}\n",
    "            for i in range(num_classes)\n",
    "        }\n",
    "\n",
    "    # 전체 성능 계산\n",
    "    fold_accuracies.append(accuracy_score(test_labels, test_probs.argmax(axis=1)))\n",
    "    fold_aucs.append(roc_auc_score(test_labels, test_probs, multi_class='ovr'))\n",
    "\n",
    "    # One-vs-Rest 방식으로 AUPR 계산\n",
    "    auprs = [\n",
    "        average_precision_score((test_labels == i).astype(int), test_probs[:, i])\n",
    "        for i in range(num_classes)\n",
    "    ]\n",
    "    fold_auprs.append(np.mean(auprs))\n",
    "\n",
    "    # Precision, Recall, F1 Score 계산\n",
    "    fold_precisions.append(precision_score(test_labels, test_probs.argmax(axis=1), average='macro'))\n",
    "    fold_recalls.append(recall_score(test_labels, test_probs.argmax(axis=1), average='macro'))\n",
    "    fold_f1_scores.append(f1_score(test_labels, test_probs.argmax(axis=1), average='macro'))\n",
    "\n",
    "    # 클래스별 메트릭 계산\n",
    "    for i in range(num_classes):\n",
    "        class_pred = (test_probs.argmax(axis=1) == i).astype(int)\n",
    "        class_true = (test_labels == i).astype(int)\n",
    "\n",
    "        class_metrics[i][\"accuracy\"].append(accuracy_score(class_true, class_pred))\n",
    "        class_metrics[i][\"auc\"].append(roc_auc_score(class_true, test_probs[:, i]))\n",
    "        class_metrics[i][\"aupr\"].append(auprs[i])\n",
    "        class_metrics[i][\"precision\"].append(precision_score(class_true, class_pred))\n",
    "        class_metrics[i][\"recall\"].append(recall_score(class_true, class_pred))\n",
    "        class_metrics[i][\"f1\"].append(f1_score(class_true, class_pred))\n",
    "\n",
    "# Fold별 평균 및 표준편차 계산 함수\n",
    "def mean_and_std(values):\n",
    "    return np.mean(values), np.std(values)\n",
    "\n",
    "# Fold별 전체 성능 출력\n",
    "print(f\"Overall Metrics Across 10 Folds:\")\n",
    "print(f\"Accuracy: {mean_and_std(fold_accuracies)[0]:.4f} ± {mean_and_std(fold_accuracies)[1]:.4f}\")\n",
    "print(f\"AUC: {mean_and_std(fold_aucs)[0]:.4f} ± {mean_and_std(fold_aucs)[1]:.4f}\")\n",
    "print(f\"AUPR: {mean_and_std(fold_auprs)[0]:.4f} ± {mean_and_std(fold_auprs)[1]:.4f}\")\n",
    "print(f\"Precision: {mean_and_std(fold_precisions)[0]:.4f} ± {mean_and_std(fold_precisions)[1]:.4f}\")\n",
    "print(f\"Recall: {mean_and_std(fold_recalls)[0]:.4f} ± {mean_and_std(fold_recalls)[1]:.4f}\")\n",
    "print(f\"F1 Score: {mean_and_std(fold_f1_scores)[0]:.4f} ± {mean_and_std(fold_f1_scores)[1]:.4f}\")\n",
    "\n",
    "# 클래스별 성능 출력\n",
    "for i in range(num_classes):\n",
    "    print(f\"\\nClass {i} Metrics Across 10 Folds:\")\n",
    "    print(f\"  Accuracy: {mean_and_std(class_metrics[i]['accuracy'])[0]:.4f} ± {mean_and_std(class_metrics[i]['accuracy'])[1]:.4f}\")\n",
    "    print(f\"  AUC: {mean_and_std(class_metrics[i]['auc'])[0]:.4f} ± {mean_and_std(class_metrics[i]['auc'])[1]:.4f}\")\n",
    "    print(f\"  AUPR: {mean_and_std(class_metrics[i]['aupr'])[0]:.4f} ± {mean_and_std(class_metrics[i]['aupr'])[1]:.4f}\")\n",
    "    print(f\"  Precision: {mean_and_std(class_metrics[i]['precision'])[0]:.4f} ± {mean_and_std(class_metrics[i]['precision'])[1]:.4f}\")\n",
    "    print(f\"  Recall: {mean_and_std(class_metrics[i]['recall'])[0]:.4f} ± {mean_and_std(class_metrics[i]['recall'])[1]:.4f}\")\n",
    "    print(f\"  F1 Score: {mean_and_std(class_metrics[i]['f1'])[0]:.4f} ± {mean_and_std(class_metrics[i]['f1'])[1]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nayb37",
   "language": "python",
   "name": "nayb37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
